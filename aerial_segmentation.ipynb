{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "base_path = os.path.join(current_dir, \"input_data\", \"dataset\", \"semantic_drone_dataset\")\n",
    "IMAGE_PATH = os.path.join(base_path, \"original_images\")\n",
    "TARGET_PATH = os.path.join(base_path, \"label_images_semantic\")\n",
    "COLOR_TARGET_PATH = os.path.join(current_dir,\"input_data\", \"RGB_color_image_masks\")\n",
    "CSV_PATH = os.path.join(current_dir, \"input_data\", \"class_dict_seg.csv\")\n",
    "\n",
    "class_dict = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_dict) - 1\n",
    "name=[]\n",
    "masks = []\n",
    "\n",
    "for dir_name, _, filenames in os.walk(IMAGE_PATH):\n",
    "    for filename in filenames:\n",
    "        name.append(filename.split('.')[0])\n",
    "        \n",
    "for dir_name, _, filenames in os.walk(TARGET_PATH):\n",
    "    for filename in filenames:\n",
    "        masks.append(filename.split('.')[0])\n",
    "\n",
    "name.sort()\n",
    "masks.sort()\n",
    "\n",
    "print('IMAGE {}, MASK {}'.format(len(masks),len(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_and_mask(image_path, mask_path, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Display an image and its corresponding mask side by side.\n",
    "    \n",
    "    Args:\n",
    "    image_path (str): Path to the original image file.\n",
    "    mask_path (str): Path to the mask image file.\n",
    "    alpha (float): Transparency of the mask overlay. Range 0-1.\n",
    "    \"\"\"\n",
    "    # Read the original image using cv2\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "    # Read the mask using PIL\n",
    "    mask = Image.open(mask_path)\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    # Create a figure with two subplots side by side\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Display the original image\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original Image')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Display the mask\n",
    "    ax2.imshow(mask)\n",
    "    ax2.set_title('Mask')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # Display the original image with the mask overlay\n",
    "    ax3.imshow(img)\n",
    "    ax3.imshow(mask, alpha=alpha)\n",
    "    ax3.set_title(f'Image with Mask Overlay (alpha={alpha})')\n",
    "    ax3.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_image_and_mask(IMAGE_PATH+\"/\"+name[0]+'.jpg',TARGET_PATH + \"/\"+masks[0]+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 23 \n",
    "\n",
    "def create_df():\n",
    "    name = []\n",
    "    for dirname, _, filenames in os.walk(IMAGE_PATH):\n",
    "        for filename in filenames:\n",
    "            name.append(filename.split('.')[0])\n",
    "    \n",
    "    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n",
    "\n",
    "df = create_df()\n",
    "print('Total Images: ', len(df))\n",
    "X_trainval , X_test = train_test_split(df['id'].values,test_size = 0.1,random_state = 42)\n",
    "X_train , X_val = train_test_split(X_trainval , test_size = 0.2 , random_state = 42)\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "class AerialDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, X, mean=None, std=None, transform=None, is_train=True):\n",
    "        self.img_path = img_path\n",
    "        self.X = X\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.mask_path = mask_path\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "        # Define default augmentations if no transform is provided\n",
    "        if self.transform is None:\n",
    "            self.transform = self.get_default_transform(is_train)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image using cv2\n",
    "        img = cv2.imread(os.path.join(self.img_path, self.X[idx] + '.jpg'))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load mask using cv2\n",
    "        mask = cv2.imread(os.path.join(self.mask_path, self.X[idx] + '.png'), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Apply transformations\n",
    "        transformed = self.transform(image=img, mask=mask)\n",
    "        img = transformed['image']\n",
    "        mask = transformed['mask']\n",
    "\n",
    "        # Convert mask to tensor\n",
    "        # Convert mask to long tensor if it's not already a tensor\n",
    "        if not isinstance(mask, torch.Tensor):\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "        else:\n",
    "            mask = mask.long()\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def get_default_transform(self, is_train):\n",
    "        if is_train:\n",
    "            return A.Compose([\n",
    "                A.Resize(1000, 1500, interpolation=cv2.INTER_NEAREST),\n",
    "                A.HorizontalFlip(), \n",
    "                A.VerticalFlip(), \n",
    "                A.GridDistortion(p=0.5),\n",
    "                A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.5),\n",
    "                A.OpticalDistortion(distort_limit=0.1, shift_limit=0.1, p=0.5),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "                A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "                A.Normalize(mean=self.mean, std=self.std) if self.mean and self.std else A.Normalize(),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "        else:\n",
    "            return A.Compose([\n",
    "                A.Resize(1000, 1500, interpolation=cv2.INTER_NEAREST),\n",
    "                A.Normalize(mean=self.mean, std=self.std) if self.mean and self.std else A.Normalize(),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "\n",
    "\n",
    "#datasets\n",
    "train_set = AerialDataset(IMAGE_PATH, TARGET_PATH, X_train,mean,std,is_train=True)\n",
    "val_set = AerialDataset(IMAGE_PATH, TARGET_PATH, X_val,mean,std)\n",
    "test_set = AerialDataset(IMAGE_PATH, TARGET_PATH, X_test,mean,std)\n",
    "\n",
    "#dataloader\n",
    "batch_size= 5\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True,pin_memory=True) \n",
    "test_loader = DataLoader(test_set, batch_size=4, shuffle=False) \n",
    "\n",
    "print(\"Number of batches in the train data loader:\", len(train_loader))\n",
    "print(\"Number of batches in the valid data loader:\", len(val_loader))\n",
    "print(\"Number of batches in the test data loader:\", len(test_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        # Use Kaiming initialization for Conv2d layers\n",
    "        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if module.bias is not None:\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "\n",
    "class OutConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class DoubleConv(torch.nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        mid_channels = out_channels // 2  # Adjust the mid_channels based on out_channels\n",
    "        self.double_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(mid_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class Up(torch.nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = DoubleConv(in_channels + skip_channels, out_channels)\n",
    "        self.residual = nn.Conv2d(in_channels + skip_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = self.up(x)\n",
    "        if skip is not None:\n",
    "            x = torch.nn.functional.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=True)\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        return self.conv(x) + self.residual(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloEncoder(torch.nn.Module):\n",
    "    def __init__(self, yolov5_model):\n",
    "        super(YoloEncoder, self).__init__()\n",
    "        self.yolov5_model = yolov5_model\n",
    "        #self.apply(initialize_weights) #Comment when using with transfer learning\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for i, layer in enumerate(self.yolov5_model.model):\n",
    "            if isinstance(layer, nn.Module):\n",
    "                if isinstance(layer, (nn.Sequential, nn.ModuleList)):\n",
    "                    # For sequential or module list, process each sub-layer\n",
    "                    for sub_layer in layer:\n",
    "                        x = sub_layer(x)\n",
    "                elif hasattr(layer, 'forward'):\n",
    "                    # Check if the layer expects a list input\n",
    "                    if 'Concat' in layer.__class__.__name__:\n",
    "                        x = layer([x, features[-1]])  # Assuming it concatenates with the previous feature\n",
    "                    else:\n",
    "                        x = layer(x)\n",
    "                features.append(x)\n",
    "            if i == 23:  # Adjust this if you need features from different layers\n",
    "                break\n",
    "        return x, features\n",
    "\n",
    "class UNetWithYoloEncoder(torch.nn.Module):\n",
    "    def __init__(self, yolov5_encoder, n_classes):\n",
    "        super(UNetWithYoloEncoder, self).__init__()\n",
    "        self.encoder = yolov5_encoder\n",
    "        self.up1 = Up(512, 512, 256)  # input = layer 23 output\n",
    "        self.up2 = Up(256, 256, 128)  # layer 20\n",
    "        self.up3 = Up(128, 128, 64)   # layer 17\n",
    "        self.up4 = Up(64, 256, 128)   # layer 13\n",
    "        self.up5 = Up(128, 512, 256)  # layer 9\n",
    "        self.up6 = Up(256, 256, 128)  # layer 6\n",
    "        self.up7 = Up(128, 128, 64)   # layer 4\n",
    "        self.up8 = Up(64, 64, 32)     # layer 2\n",
    "        self.out_conv = torch.nn.Conv2d(32, n_classes, kernel_size=1)  # Update input channels\n",
    "        self.apply(self._init_decoder_weights)\n",
    "\n",
    "    def _init_decoder_weights(self, m):\n",
    "        if isinstance(m, (Up, OutConv)):\n",
    "            initialize_weights(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_size = x.shape[2:]\n",
    "        x,features = self.encoder(x)\n",
    "        # Follow layers given above\n",
    "        x = self.up1(x,features[23])\n",
    "        x = self.up2(x, features[20])\n",
    "        x = self.up3(x, features[17])\n",
    "        x = self.up4(x, features[13])\n",
    "        x = self.up5(x, features[9])\n",
    "        x = self.up6(x, features[6])\n",
    "        x = self.up7(x, features[4])\n",
    "        x = self.up8(x, features[2])\n",
    "        \n",
    "        x = self.out_conv(x)\n",
    "        x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=True)\n",
    "        return x\n",
    "\n",
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n",
    "\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Ensure predictions and targets have the same shape\n",
    "        assert predictions.shape == targets.shape, f\"Predictions shape {predictions.shape} doesn't match targets shape {targets.shape}\"\n",
    "        \n",
    "        # Flatten predictions and targets\n",
    "        predictions = predictions.reshape(-1)\n",
    "        targets = targets.reshape(-1)\n",
    "\n",
    "        intersection = (predictions * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, weight=0.5, num_classes=23):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Convert predictions to probabilities\n",
    "        pred_probs = F.softmax(predictions, dim=1)\n",
    "        \n",
    "        # Create one-hot encoded target\n",
    "        targets_one_hot = F.one_hot(targets.squeeze(1).long(), num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # Calculate Dice loss for each class and average\n",
    "        dice_loss = 0\n",
    "        for i in range(self.num_classes):\n",
    "            dice_loss += self.dice_loss(pred_probs[:, i], targets_one_hot[:, i])\n",
    "        dice_loss /= self.num_classes\n",
    "\n",
    "        ce = self.ce_loss(predictions, targets.squeeze(1).long())\n",
    "        # print(f\"Dice Loss: {dice_loss.item()}, CE Loss: {ce.item()}\")\n",
    "        return (self.weight * dice_loss) + ((1 - self.weight) * ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0.0001, path='YoloUnetV4.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model to {self.path} ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization code for images, target masks, and predictions masks\n",
    "\n",
    "class_names = class_dict['name'].tolist()\n",
    "\n",
    "def create_color_map(class_dict):\n",
    "    color_map = {}\n",
    "    for _, row in class_dict.iterrows():\n",
    "        class_name = row['name']\n",
    "        rgb = [row[' r'] / 255.0, row[' g'] / 255.0, row[' b'] / 255.0]  # Normalize RGB values to [0, 1]\n",
    "        color_map[class_name] = rgb\n",
    "    return color_map\n",
    "\n",
    "color_map = create_color_map(class_dict)\n",
    "\n",
    "def apply_color_map(mask, color_map, class_dict):\n",
    "    rgb_mask = np.zeros((*mask.shape, 3))\n",
    "    for class_name, color in color_map.items():\n",
    "        class_index = class_dict[class_dict['name'] == class_name].index[0]\n",
    "        rgb_mask[mask == class_index] = color\n",
    "    return rgb_mask\n",
    "\n",
    "\n",
    "def plot_results(images, true_masks, pred_masks, n_classes, color_map, class_dict, batch_size=12):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    \n",
    "    # Loop thorugh first 3 images in batch (change depending on batch size)\n",
    "    for i in range(3):\n",
    "        # Display input image\n",
    "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0, 1]\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title('Input Image')\n",
    "        \n",
    "        # Display true mask\n",
    "        true_mask = true_masks[i].cpu().numpy()\n",
    "        true_mask_rgb = apply_color_map(true_mask, color_map, class_dict)\n",
    "        axes[i, 1].imshow(true_mask_rgb)\n",
    "        axes[i, 1].set_title('True Mask')\n",
    "        \n",
    "        # Display predicted mask\n",
    "        pred_mask = pred_masks[i].cpu().numpy()\n",
    "        pred_mask_rgb = apply_color_map(pred_mask, color_map, class_dict)\n",
    "        axes[i, 2].imshow(pred_mask_rgb)\n",
    "        axes[i, 2].set_title('Predicted Mask')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_color_map(color_map, class_dict):\n",
    "    num_classes = len(color_map)\n",
    "    fig, ax = plt.subplots(figsize=(12, num_classes * 0.5))\n",
    "    \n",
    "    for i, (class_name, color) in enumerate(color_map.items()):\n",
    "        rect = plt.Rectangle((0, i), 1, 1, facecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(1.1, i + 0.5, class_name, va='center')\n",
    "    \n",
    "    ax.set_xlim(0, 2)\n",
    "    ax.set_ylim(0, num_classes)\n",
    "    ax.axis('off')\n",
    "    plt.title('Color Map Verification')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the color map\n",
    "color_map = create_color_map(class_dict)\n",
    "\n",
    "# Visualize the color map\n",
    "visualize_color_map(color_map, class_dict)\n",
    "\n",
    "# Print out the RGB values for each class\n",
    "print(\"Class Name: (R, G, B)\")\n",
    "for class_name, color in color_map.items():\n",
    "    r, g, b = [int(c * 255) for c in color]  # Convert back to 0-255 range for readability\n",
    "    print(f\"{class_name}: ({r}, {g}, {b})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize Models and Criteria\n",
    "num_classes = len(class_dict) - 1\n",
    "\n",
    "# YOLO-Encoder\n",
    "# yolov5_model = YOLO('yolov5su.pt')  # or any other YOLOv5 model variant\n",
    "yolov5_model = torch.load(os.getcwd()+'/yolov5su.pt', map_location=device)['model']\n",
    "yolov5_model = yolov5_model.to(torch.float32)  # Convert to float32\n",
    "yolo_encoder = YoloEncoder(yolov5_model).to(device)\n",
    "\n",
    "# UNET w/ Encoder\n",
    "model = UNetWithYoloEncoder(yolo_encoder, n_classes=num_classes)\n",
    "model.to(device)\n",
    "# for param in model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Criteria\n",
    "loss_fn = CombinedLoss(0.75) # Set higher to give more weight to dice loss component, lower gives more weight to cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min', patience=3,factor=0.1)\n",
    "early_stopping = EarlyStopping(patience=8, verbose=True,path='YoloUnetV7.pt')\n",
    "scaler = GradScaler()\n",
    "num_epochs = 75  # Set the number of epochs\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_iou = []; val_acc = []\n",
    "train_iou = []; train_acc = []\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    model.train()  \n",
    "    train_loss = 0.0\n",
    "    iou_score = 0\n",
    "    train_accuracy = 0\n",
    "\n",
    "    for i, (inputs, masks) in enumerate(train_loader): \n",
    "        images, masks = inputs.to(device), masks.to(device)  # Send inputs to GPU if available\n",
    "        # optimizer.zero_grad()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # predictions = model(images)\n",
    "        # # loss_masks =  masks.squeeze(1)\n",
    "        # loss = loss_fn(predictions,masks) \n",
    "        with autocast():\n",
    "            predictions = model(images)\n",
    "            loss = loss_fn(predictions, masks) \n",
    "\n",
    "        # loss.backward()\n",
    "        # # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # optimizer.step()\n",
    "        \n",
    "        # Use the scaler to scale the loss and call backward\n",
    "        scaler.scale(loss).backward()\n",
    "        # Unscale gradients and call or skip optimizer.step()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        with torch.no_grad():\n",
    "            iou_score += mIoU(predictions, masks)\n",
    "            train_accuracy += pixel_accuracy(predictions, masks)        \n",
    "\n",
    "    epoch_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Start validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0\n",
    "    val_iou_score = 0\n",
    "    with torch.no_grad():  # No gradient computation in validation phase\n",
    "        for i, (val_inputs, val_masks) in enumerate(val_loader):\n",
    "            val_inputs, val_masks = val_inputs.to(device), val_masks.to(device)\n",
    "            # val_loss_masks = val_masks.squeeze(1).long()\n",
    "\n",
    "            with autocast():\n",
    "                val_predictions = model(val_inputs)\n",
    "                loss = loss_fn(val_predictions, val_masks)\n",
    "\n",
    "            val_iou_score +=  mIoU(val_predictions,val_masks)\n",
    "            val_accuracy += pixel_accuracy(val_predictions, val_masks)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "            # Visualization and printing code...\n",
    "            # if epoch % 10 == 0 and i % 20== 0:\n",
    "            #     # Convert the batch to CPU and detach it\n",
    "            #     inputs_cpu = images.cpu().detach()\n",
    "            #     masks_cpu = masks.cpu().detach()\n",
    "            #     predictions_cpu = predictions.cpu().detach()\n",
    "            #     pred_masks = torch.argmax(predictions_cpu, dim=1)\n",
    "            #     plot_results(inputs_cpu,masks_cpu,pred_masks,num_classes,color_map,class_dict)\n",
    "            #     # Check Gradient Norm\n",
    "            #     total_norm = 0\n",
    "            #     for p in model.parameters():\n",
    "            #         if p.grad is not None:\n",
    "            #             param_norm = p.grad.data.norm(2)\n",
    "            #             total_norm += param_norm.item() ** 2\n",
    "            #     total_norm = total_norm ** 0.5\n",
    "            #     print(f\"Gradient norm: {total_norm}\")\n",
    "            #     # print(f\"Batch {i}, Epoch {epoch+1}\")\n",
    "            #     # # print(f\"Mask min and max: {masks.min()}, {masks.max()}\")\n",
    "            #     # print(f\"Unique values in masks: {torch.unique(masks)}\")\n",
    "            #     # print(f\"Unique values in predictions: {torch.unique(torch.argmax(predictions, dim=1))}\")\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_iou.append(val_iou_score/len(val_loader))\n",
    "        train_iou.append(iou_score/len(train_loader))\n",
    "        train_acc.append(train_accuracy/len(train_loader))\n",
    "        val_acc.append(val_accuracy/ len(val_loader))\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    epoch_minutes = int(epoch_time // 60)\n",
    "    epoch_seconds = int(epoch_time % 60)\n",
    "\n",
    "    print(\"\\nEpoch:{}/{} |\".format(epoch+1, num_epochs),\n",
    "            \"Train Loss: {:.3f} |\".format(epoch_loss),\n",
    "            \"Val Loss: {:.3f} |\".format(avg_val_loss),\n",
    "            \"Train mIoU:{:.3f} |\".format(iou_score/len(train_loader)),\n",
    "            \"Val mIoU: {:.3f} |\".format(val_iou_score/len(val_loader)),\n",
    "            \"Train Acc:{:.3f} |\".format(train_accuracy/len(train_loader)),\n",
    "            \"Val Acc:{:.3f} |\".format(val_accuracy/len(val_loader)),\n",
    "            \"Time: {:02d}:{:02d} mins\".format(epoch_minutes,epoch_seconds))\n",
    "\n",
    "    \n",
    "    # Check Gradient Norm\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    print(f\"Gradient norm: {total_norm}\")\n",
    "\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "    print(f'Learning Rate: {scheduler.get_last_lr()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(train_iou, label='Training mIoU')\n",
    "plt.plot(val_iou, label='Validation mIoU')\n",
    "plt.title('mIoU over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('mIoU')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('combined_metrics_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('YoloUnetV7.pt'))\n",
    "color_map = create_color_map(class_dict)\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_accuracy = 0\n",
    "test_iou_score = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, masks in test_loader:\n",
    "        images, masks = inputs.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, masks)\n",
    "\n",
    "        # Visualize Reults\n",
    "        inputs_cpu = images.cpu().detach()\n",
    "        masks_cpu = masks.cpu().detach()\n",
    "        predictions_cpu = outputs.cpu().detach()\n",
    "        pred_masks = torch.argmax(predictions_cpu, dim=1)\n",
    "        plot_results(inputs_cpu,masks_cpu,pred_masks,num_classes,color_map,class_dict)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_iou_score += mIoU(outputs, masks)\n",
    "        test_accuracy += pixel_accuracy(outputs, masks)\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "avg_test_iou = test_iou_score / len(test_loader)\n",
    "avg_test_accuracy = test_accuracy / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test mIoU: {avg_test_iou:.4f}\")\n",
    "print(f\"Test Accuracy: {avg_test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
